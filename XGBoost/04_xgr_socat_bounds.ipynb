{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description\n",
    "\n",
    "XGBoostRegression to find Uncertainty Bounds\n",
    "\n",
    "Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# For accessing directories\n",
    "# =========================================\n",
    "root_dir = \"/data/artemis/workspace/vbennington/SOCAT_ML/pCO2_DIC\"\n",
    "\n",
    "reference_output_dir = f\"{root_dir}/references\"\n",
    "data_output_dir = f\"{root_dir}/data/processed\"\n",
    "data_output_dir = f\"/data/artemis/workspace/vbennington/SOCAT_ML/pCO2_DIC/data/processed\"\n",
    "model_output_dir = f\"{root_dir}/models/trained\"\n",
    "recon_output_dir = f\"{root_dir}/models/reconstructions\"\n",
    "other_output_dir = f\"{root_dir}/models/performance_metrics\"\n",
    "\n",
    "approach = 'xgr'\n",
    "approach_output_dir = f\"{other_output_dir}/{approach}\"\n",
    "# =========================================\n",
    "# Number of cores you have access to for model training\n",
    "# =========================================\n",
    "jobs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'jpg'\n",
    "%config InlineBackend.print_figure_kwargs = {'dpi':150, 'bbox_inches': 'tight'}\n",
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sklearn.linear_model \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "import random\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# machine learning libraries\n",
    "import sklearn            # machine-learning libary with many algorithms implemented\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb     # extreme gradient boosting (XGB\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import sklearn.model_selection as mselect\n",
    "\n",
    "# Python file with supporting functions\n",
    "import pre_SOCAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predefined Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Defining some inputs for the modeling process\n",
    "# =========================================\n",
    "\n",
    "# Parameter grids\n",
    "xg_param_grid = {\"n_estimators\":[1000, 2000, 3000],\n",
    "                 \"max_depth\":[4,5,6,7,8,9]\n",
    "                }\n",
    "\n",
    "# Feature and target lists for feeding into ML\n",
    "features_sel = ['sst','sst_anom','sss','sss_anom','chl_log','chl_anom','mld_log','xco2','A','B','C','T0','T1'] # pCO2_DIC\n",
    "\n",
    "target_sel = ['pCO2_DIC']  # What we reconstruct with ML\n",
    "final_sel = ['pCO2'] # What we want RMSE, etc. for (pCO2 = pCO2_DIC + pCO2_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to train with 4 out of every 5 months, and test on the fifth month\n",
    "date_range_start = '1982-01-01T00:00:00.000000000'\n",
    "date_range_end = '2019-12-01T00:00:00.000000000'\n",
    "\n",
    "# create date vector\n",
    "dates = pd.date_range(start=date_range_start, \n",
    "                      end=date_range_end,freq='MS') + np.timedelta64(14, 'D')\n",
    "select_dates = []\n",
    "test_dates = []\n",
    "for i in range(0,len(dates)):\n",
    "    if i % 5 != 0:\n",
    "        select_dates.append(dates[i])\n",
    "    if i % 5 == 0:\n",
    "        test_dates.append(dates[i])\n",
    "year_mon = []\n",
    "for i in range(0,len(select_dates)):\n",
    "    tmp = select_dates[i]\n",
    "    year_mon.append(f\"{tmp.year}-{tmp.month}\")\n",
    "test_year_mon = []\n",
    "for i in range(0,len(test_dates)):\n",
    "    tmp = test_dates[i]\n",
    "    test_year_mon.append(f\"{tmp.year}-{tmp.month}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "approach = 'xgr'\n",
    "approach_output_dir = f\"{other_output_dir}/{approach}\"\n",
    "random_seeds = [1,71] # pCO2_DIC\n",
    "#random_seeds = [68, 23] # pCO2_DIC_train2000s\n",
    "#random_seeds = [50, 17] # pCO2_DIC_noCHL\n",
    "\n",
    "first_run = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-21 13:04:40.273121\n",
      "SOCAT length = 262714\n",
      "Train/Val length =  209964\n",
      "Test length = 52750\n",
      "Have X\n",
      "{'max_depth': 9, 'n_estimators': 1000}\n",
      "Doing y_upper\n",
      "Doing y_lower\n",
      "Starting model saving process\n",
      "Save complete\n",
      "Doing Y Test\n",
      "{'mse': 274.5420439730508, 'mae': 11.148414845294308, 'medae': 7.606292267503843, 'max_error': 186.197911194561, 'bias': -0.0230382592201539, 'r2': 0.835793306657876, 'corr': 0.9142995182242403, 'cent_rmse': 16.569294288281046, 'stdev': 37.885773063576075, 'amp_ratio': 0.9912416235423145, 'stdev_ref': 40.88923670469988, 'range_ref': 399.4094490875581, 'iqr_ref': 44.52784536195463}\n",
      "{'mse': 274.5420439730508, 'mae': 11.14841484529431, 'medae': 7.606292267503818, 'max_error': 186.19791119456096, 'bias': -0.02303825922018632, 'r2': 0.78988683767585, 'corr': 0.8891469467108202, 'cent_rmse': 16.569294288281046, 'stdev': 33.09338863555198, 'amp_ratio': 1.0146784805167066, 'stdev_ref': 36.14746132635529, 'range_ref': 376.92213983805937, 'iqr_ref': 43.41449354886481}\n"
     ]
    }
   ],
   "source": [
    "if first_run:\n",
    "    best_params = {} # Uncomment if running cross validation to find best params\n",
    "else:\n",
    "    param_fname = f\"/data/artemis/workspace/vbennington/SOCAT_ML/pCO2_DIC/models/performance_metrics/xgr/{approach}_best_params_dict.pickle\"\n",
    "    with open(param_fname, 'rb') as handle:\n",
    "        best_params = pickle.load(handle)\n",
    "    print(best_params)\n",
    "    \n",
    "test_performance = defaultdict(dict)\n",
    "\n",
    "K_folds = 3\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "# Data file path\n",
    "data_dir = f\"{data_output_dir}\"\n",
    "fname = f\"data_clean_2D_mon_1x1_198201-201912.pkl\"\n",
    "file_path = f\"{data_dir}/{fname}\"\n",
    "        \n",
    "# Read in data, create some selection filters, produce a reduced dataframe\n",
    "df = pd.read_pickle(file_path)\n",
    "\n",
    "# Get rid of features we never use:\n",
    "df = df.drop(columns=['chl','mld'])\n",
    "\n",
    "# Test on every fifth month to reduce autocorrelation along cruise tracks, but still sample many climates...\n",
    "df['year'] = df.index.get_level_values('time').year\n",
    "df['mon'] = df.index.get_level_values('time').month\n",
    "df['year_month'] = df['year'].astype(str) + \"-\" + df['mon'].astype(str)\n",
    "\n",
    "# Get rid of N    \n",
    "recon_sel = (~df[features_sel+['net_mask']].isna().any(axis=1))   # Dont' have dpCO2 for non-SOCAT locations, but have features\n",
    "\n",
    "# Try (200,-250) as cut off based on histogram plots of SOCAT data:\n",
    "################################################################################################################################        \n",
    "sel = (recon_sel & (df['socat_mask'] == 1)) & ((df[target_sel] < 300) & ((df[target_sel] > -300))).to_numpy().ravel()           # locations not masked AND IN SOCAT SAMPLING and within reason\n",
    "\n",
    "print(\"SOCAT length =\",sum(sel))\n",
    "\n",
    "###################################################################################################################################\n",
    "# Separate the data sets\n",
    "###################################################################################################################################\n",
    "\n",
    "train_val_sel = ((sel) & (pd.Series(df['year_month']).isin(year_mon))).to_numpy().ravel()\n",
    "print(\"Train/Val length = \",sum(train_val_sel))\n",
    "\n",
    "test_sel = ((sel) & (pd.Series(df['year_month']).isin(test_year_mon))).to_numpy().ravel()   # Should be along SOCAT track\n",
    "print(\"Test length =\",sum(test_sel))     \n",
    "\n",
    "################################################################################################################################\n",
    "        \n",
    "# Convert dataframe to numpy arrays, train/val/test split\n",
    "X = df.loc[sel,features_sel].to_numpy()         \n",
    "y = df.loc[sel,target_sel].to_numpy().ravel()\n",
    "\n",
    "# Where we want pCO2 reconstructed\n",
    "X_recon = df.loc[recon_sel,features_sel].to_numpy()         \n",
    "\n",
    "\n",
    "#X_train, X_val, y_train, y_val = mselect.train_test_split(X_train_val, y_train_val, test_size=val_prop, random_state=42)  \n",
    "# Convert dataframe to numpy arrays, train/val/test split\n",
    "X_train_val = df.loc[train_val_sel,features_sel].to_numpy()                # create Xtrain and Xtest to randomly select from for X_train and X_test\n",
    "y_train_val = df.loc[train_val_sel,target_sel].to_numpy().ravel()\n",
    "\n",
    "\n",
    "print(\"Have X\")\n",
    "\n",
    "if first_run:\n",
    "    \n",
    "    # Define the model with validation set #####  \n",
    "    model = GradientBoostingRegressor(random_state=random_seeds[0])\n",
    "    param_grid = xg_param_grid\n",
    "    grid = GridSearchCV(model, param_grid, cv=K_folds, return_train_score=False, refit=True)\n",
    "    grid.fit(X_train_val,y_train_val)\n",
    "    best_params = grid.best_params_\n",
    "    print(best_params)\n",
    "\n",
    "# Fit the model on train/validation data for upper bound:\n",
    "model = GradientBoostingRegressor(loss='quantile',random_state=random_seeds[1], **best_params, alpha=0.95)\n",
    "model.fit(X_train_val, y_train_val)          \n",
    "\n",
    "print(\"Doing y_upper\")\n",
    "# Now, apply everywhere with this upper bound estimate:\n",
    "y_upper = model.predict(X_recon)\n",
    "                                               \n",
    "# Now get lower bounds:\n",
    "model.set_params(alpha=1-0.95)                                               \n",
    "model.fit(X_train_val, y_train_val)\n",
    "print(\"Doing y_lower\")\n",
    "y_lower = model.predict(X_recon)                                               \n",
    "                                               \n",
    "# Now get median prediction:\n",
    "model.set_params(loss='ls')\n",
    "model.fit(X_train_val, y_train_val)\n",
    "                                                                            \n",
    "# Save this model:\n",
    "pre_SOCAT.save_model(model, model_output_dir, approach)   #Uncomment when actually running\n",
    "\n",
    "################ TEST ####################################################################################################\n",
    "X_test = df.loc[test_sel,features_sel].to_numpy()                #  Test metrics on all of SOCAT data from test years\n",
    "y_test = df.loc[test_sel,target_sel].to_numpy().ravel()    \n",
    "        \n",
    "# Calculate some test error metrics and store in a dictionary\n",
    "print(\"Doing Y Test\")\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# for pCO2_DIC\n",
    "test_performance = pre_SOCAT.evaluate_test(y_test, y_pred_test)\n",
    "print(test_performance)\n",
    "\n",
    "# for pCO2\n",
    "y_final = df.loc[test_sel,final_sel].to_numpy().ravel()  # Real pCO2 from SOCAT\n",
    "y_pco2t = df.loc[test_sel,'pCO2_T'].to_numpy().ravel() # pCO2_T\n",
    "y_pred_final = y_pred_test + y_pco2t\n",
    "\n",
    "pco2_performance = pre_SOCAT.evaluate_test(y_final, y_pred_final)\n",
    "print(pco2_performance)\n",
    "######################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting reconstruction saving process\n",
      "Save complete\n",
      "2021-06-22 11:20:06.559532\n"
     ]
    }
   ],
   "source": [
    "# Make room for our reconstruction:\n",
    "df=df.drop(columns=['sss','sst','mld_log','chl_log','sss_anom','sst_anom','A','B','C','T0','T1'])\n",
    "\n",
    "# Reconstruct where no data ##########################################################\n",
    "#y_pred_nonsocat = model.predict(X_nonsocat)\n",
    "\n",
    "# Everywhere ####################################################\n",
    "y_recon = model.predict(X_recon)\n",
    "\n",
    "# Full reconstruction ##\n",
    "df['pCO2_DIC_recon'] = np.nan\n",
    "df.loc[recon_sel,['pCO2_DIC_recon']] = y_recon   \n",
    "\n",
    "# Full reconstruction Lower ##\n",
    "df['pCO2_DIC_lower_recon'] = np.nan\n",
    "df.loc[recon_sel,['pCO2_DIC_lower_recon']] = y_lower \n",
    "\n",
    "# Full reconstruction Upper ##\n",
    "df['pCO2_DIC_upper_recon'] = np.nan\n",
    "df.loc[recon_sel,['pCO2_DIC_upper_recon']] = y_upper \n",
    "\n",
    "# Full reconstruction ##\n",
    "df['pCO2_recon'] = np.nan\n",
    "df.loc[recon_sel,['pCO2_recon']] = y_recon + df.loc[recon_sel,'pCO2_T'].to_numpy() # pCO2-DIC + pCO2_T\n",
    "\n",
    "df['pCO2_test_recon'] = np.nan\n",
    "df.loc[test_sel,['pCO2_test_recon']] = y_pred_final\n",
    "\n",
    "df['pCO2_test'] = np.nan\n",
    "df.loc[test_sel,['pCO2_test']] = y_final\n",
    "\n",
    "df['pCO2_DIC_test_recon'] = np.nan\n",
    "df.loc[test_sel,['pCO2_DIC_test_recon']] = y_pred_test\n",
    "\n",
    "df['pCO2_DIC_test'] = np.nan\n",
    "df.loc[test_sel,['pCO2_DIC_test']] = y_test\n",
    "\n",
    "        \n",
    "DS_recon = df[['net_mask','socat_mask','pCO2_DIC','pCO2_DIC_upper_recon','pCO2_DIC_lower_recon','pCO2_DIC_recon','pCO2','pCO2_recon','pCO2_T','pCO2_DIC_test','pCO2_DIC_test_recon','pCO2_test','pCO2_test_recon']].to_xarray()\n",
    "\n",
    "########## SAVE ####################################################################################################\n",
    "pre_SOCAT.save_recon(DS_recon, recon_output_dir, approach)   # Uncomment when actually running\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best parameters and performance metrics\n",
    "approach_output_dir = f\"{other_output_dir}/{approach}\"\n",
    "param_fname = f\"{approach_output_dir}/{approach}_best_params_dict.pickle\"\n",
    "test_perform_fname = f\"{approach_output_dir}/{approach}_test_performance_dict.pickle\"\n",
    "\n",
    "Path(approach_output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(param_fname, 'wb') as handle:\n",
    "    pickle.dump(best_params, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(test_perform_fname, 'wb') as handle:\n",
    "    pickle.dump(test_performance, handle)\n",
    "    \n",
    "# Convert performance metrics to dataframes\n",
    "test_df = pd.DataFrame.from_dict(test_performance,\n",
    "                                 orient='index')\n",
    "\n",
    "# Save the dataframes too\n",
    "test_df_fname = f\"{approach_output_dir}/{approach}_test_performance_df.pickle\"\n",
    "\n",
    "test_df.to_pickle(test_df_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EOF Test2",
   "language": "python",
   "name": "dev2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
